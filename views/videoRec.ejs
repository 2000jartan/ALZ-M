

<head>
	<title>test</title>
	<style type="text/css">
		body{
			margin: 0;
			padding: 0;
			width: 100vw;
			display: flex;
			height: 100vh;
			justify-content: center;
			align-items: center;
			flex-direction: column;
		}
		canvas{
			
			position: absolute;
      
		}
	</style>
</head>
<body>

	<video id="videoInput" width="720" height="550" controls muted></video>


	<script defer async src="./js/face-api.min.js"></script>

    <script defer async >
    	const video = document.getElementById('videoInput')


Promise.all([
    faceapi.nets.faceRecognitionNet.loadFromUri('/models'),
    faceapi.nets.faceLandmark68Net.loadFromUri('/models'),
    faceapi.nets.ssdMobilenetv1.loadFromUri('/models') //heavier/accurate version of tiny face detector
]).then(start)


function start() {
    document.body.append('Loaded');
    //video.src = '../videos/speech.mp4';

    navigator.getUserMedia(
        { video:{} },
        stream => video.srcObject = stream,
        err => console.error(err)
    )

    recognizeFaces();
}

async function recognizeFaces() {

    const labeledDescriptors = await loadLabeledImages()
    console.log(labeledDescriptors)
    const faceMatcher = new faceapi.FaceMatcher(labeledDescriptors, 0.7)


    video.addEventListener('play', async () => {

    console.log('Playing')
        const canvas = faceapi.createCanvasFromMedia(video)
        document.body.append(canvas)

        const displaySize = { width: video.width, height: video.height }
        faceapi.matchDimensions(canvas, displaySize)

        setInterval(async () => {
            const detections = await faceapi.detectAllFaces(video).withFaceLandmarks().withFaceDescriptors()

            const resizedDetections = faceapi.resizeResults(detections, displaySize)

            canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height)

            const results = resizedDetections.map((d) => {
                return faceMatcher.findBestMatch(d.descriptor)
            })
            results.forEach( (result, i) => {
                const box = resizedDetections[i].detection.box
                const drawBox = new faceapi.draw.DrawBox(box, { label: result.toString() })
                drawBox.draw(canvas)
            })
        }, 100);


        



        
    })

}


function loadLabeledImages() {
    //const labels = ['Black Widow', 'Captain America', 'Captain Marvel', 'Hawkeye' , 'Jim Rhodes', 'Thor', 'Tony Stark']
    //const labels = ['Vishaka Mohan'] 
    const labels = <% labels %>;
    let path = <% path %>;
    let j = 0;
    return Promise.all(
        labels.map(async (label)=>{
            const descriptions = []
            for(let i=1; i<=path[j].length; i++) {
                const img = await faceapi.fetchImage(path[j][i]);
                const detections = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor();
                console.log(label + i + JSON.stringify(detections));
                descriptions.push(detections.descriptor);
                j++;
            }
            document.body.append(label + ' Faces Loaded | ')
            return new faceapi.LabeledFaceDescriptors(label, descriptions)
        })
    )
}

  
    </script>
</body>
